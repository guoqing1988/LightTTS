# Adapted from vllm/entrypoints/api_server.py
# of the vllm-project/vllm GitHub repository.
#
# Copyright 2023 ModelTC Team
# Copyright 2023 vLLM Team
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import io
import asyncio
import traceback
import torch
import uvloop
import hashlib
import base64
import subprocess
import time
import copy
import os
from pathlib import Path
import sys

# æŠ‘åˆ¶å¸¸è§çš„åº“è­¦å‘Š
import warnings

warnings.filterwarnings("ignore", category=FutureWarning, module="diffusers")
warnings.filterwarnings("ignore", category=UserWarning, message=".*weight_norm.*")
warnings.filterwarnings("ignore", category=UserWarning, module="onnxruntime")

sys.path.insert(0, str(Path(__file__).parent.parent.parent / "cosyvoice"))
from light_tts.utils.load_utils import CosyVoiceVersion, load_yaml_frontend

asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
import argparse
import json
from http import HTTPStatus
import numpy as np
from fastapi import FastAPI, UploadFile, Form, File, BackgroundTasks, Request, WebSocketDisconnect, WebSocket
from fastapi.responses import Response, StreamingResponse, JSONResponse
import uvicorn
import soundfile as sf
from io import BytesIO
import multiprocessing as mp

from .httpserver.manager import HttpServerManager
from .tts_encode.manager import start_tts1_encode_process
from .tts_llm.manager import start_tts_llm_process
from .tts_decode.manager import start_tts_decode_process
from .req_id_generator import ReqIDGenerator

from light_tts.utils.net_utils import alloc_can_use_network_port
from light_tts.utils.param_utils import check_request
from light_tts.utils.health_utils import health_check
from light_tts.static_config import dict_language
from cosyvoice.utils.file_utils import load_wav
from light_tts.utils.envs_utils import get_env_start_args, get_unique_server_name
from light_tts.server.core.objs.sampling_params import SamplingParams
from light_tts.utils.start_utils import process_manager
from .metrics import histogram_timer
from cosyvoice.cli.frontend import CosyVoiceFrontEnd
from light_tts.utils.process_check import is_process_active
from prometheus_client import Counter, Histogram, generate_latest

all_request_counter = Counter("lightllm_request_count", "The total number of requests")
failure_request_counter = Counter("lightllm_request_failure", "The total number of requests")
sucess_request_counter = Counter("lightllm_request_success", "The total number of requests")
request_latency_histogram = Histogram("lightllm_request_latency", "Request latency", ["route"])
from dataclasses import dataclass
from .health_monitor import start_health_check_process
from light_tts.utils.log_utils import init_logger
from light_tts.server import TokenLoad


logger = init_logger(__name__)
g_id_gen = ReqIDGenerator()
lora_styles = ["CosyVoice2"]


@dataclass
class G_Objs:
    app: FastAPI = None
    args: object = None
    httpserver_manager: HttpServerManager = None
    shared_token_load: TokenLoad = None

    def set_args(self, args):
        self.args = args
        self.httpserver_manager = HttpServerManager(
            args, httpserver_port=args.httpserver_port, tts1_encode_ports=args.tts1_encode_ports
        )
        self.shared_token_load = TokenLoad(f"{get_unique_server_name()}_shared_token_load", 1)
        configs = load_yaml_frontend(args.model_dir)
        if configs["cosyvoice_version"] == CosyVoiceVersion.VERSION_2:
            speech_tokenizer_model = "{}/speech_tokenizer_v2.onnx".format(args.model_dir)
        else:
            speech_tokenizer_model = "{}/speech_tokenizer_v3.onnx".format(args.model_dir)
        self.frontend = CosyVoiceFrontEnd(
            configs["get_tokenizer"],
            configs["feat_extractor"],
            "{}/campplus.onnx".format(args.model_dir),
            speech_tokenizer_model,
            "{}/spk2info.pt".format(args.model_dir),
            configs["allowed_special"],
        )
        del self.frontend.feat_extractor
        del self.frontend.campplus_session
        del self.frontend.speech_tokenizer_session
        if torch.cuda.is_available():
            torch.cuda.empty_cache()


g_objs = G_Objs()
app = FastAPI()
g_objs.app = app


def create_error_response(status_code: HTTPStatus, message: str) -> JSONResponse:
    return JSONResponse({"message": message}, status_code=status_code.value)


# å»ºè®®æ¢æµ‹é¢‘ç‡ä¸º60sçš„é—´éš”ï¼ŒåŒæ—¶è®¤ä¸ºæ¢æµ‹å¤±è´¥çš„è¶…æ—¶æ—¶é—´ä¸º60s.è¿ç»­3æ¬¡æ¢æµ‹å¤±è´¥åˆ™é‡å¯å®¹å™¨ã€‚
@app.get("/healthz")
@app.get("/health")
async def healthcheck():
    if os.environ.get("DEBUG_HEALTHCHECK_RETURN_FAIL") == "true":
        return JSONResponse({"message": "Error"}, status_code=404)

    if await health_check(g_objs.httpserver_manager, g_id_gen, lora_styles):
        return JSONResponse({"message": "Ok"}, status_code=200)
    else:
        return JSONResponse({"message": "Error"}, status_code=404)


@app.get("/liveness")
def liveness():
    return {"status": "ok"}


@app.get("/readiness")
def readiness():
    return {"status": "ok"}


def generate_data(model_output):
    for i in model_output:
        tts_audio = (i["tts_speech"] * (2 ** 15)).astype(np.int16).tobytes()
        yield tts_audio


async def generate_data_stream(generate_objs):
    for generator in generate_objs:
        async for i in generator:
            tts_audio = (i["tts_speech"] * (2 ** 15)).astype(np.int16).tobytes()
            yield tts_audio


def calculate_md5(file: UploadFile) -> str:
    hash_md5 = hashlib.md5()
    while chunk := file.read(8192):  # åˆ†å—è¯»å–ä»¥æ”¯æŒå¤§æ–‡ä»¶
        hash_md5.update(chunk)
    return hash_md5.hexdigest()


async def send_wav(websocket: WebSocket, generator):
    async for result in generator:
        if isinstance(result, dict):
            await websocket.send_bytes((result["tts_speech"] * (2 ** 15)).astype(np.int16).tobytes())
            continue

        websocket.close()
        return


@app.websocket("/inference_zero_shot_bistream")
async def inference_zero_shot_bistream(websocket: WebSocket):
    global lora_styles
    # æ¥å— WebSocket è¿æ¥
    await websocket.accept()

    # æ¥æ”¶åˆå§‹åŒ–å‚æ•°
    init_params = await websocket.receive_json()

    # è§£æå›ºå®šå‚æ•°
    prompt_text = init_params.get("prompt_text")
    tts_model_name = init_params.get("tts_model_name", "default")

    # å¤„ç†éŸ³é¢‘æ–‡ä»¶ï¼ˆå‡è®¾å®¢æˆ·ç«¯å‘é€ base64 ç¼–ç çš„éŸ³é¢‘ï¼‰
    prompt_wav_data = await websocket.receive_bytes()
    wav_bytes_io = BytesIO(prompt_wav_data)
    prompt_speech_16k = load_wav(wav_bytes_io, 16000)
    semantic_len = (prompt_speech_16k.shape[1] + 239) // 640 + 10  # + 10 for safe

    wav_bytes_io.seek(0)
    speech_md5 = calculate_md5(wav_bytes_io)

    if tts_model_name == "default":
        tts_model_name = lora_styles[0]

    speech_index, have_alloc = g_objs.httpserver_manager.alloc_speech_mem(speech_md5, prompt_speech_16k)
    request_id = g_id_gen.generate_id()
    first_text = True
    prompt_text = g_objs.frontend.text_normalize(prompt_text, split=False)
    process_task = None
    sample_params_dict = {}
    sampling_params = SamplingParams()
    sampling_params.init(**sample_params_dict)
    sampling_params.verify()
    try:
        while True:
            input_data = await websocket.receive_json()
            tts_text = input_data.get("tts_text", "")
            cur_req_dict = {
                "text": tts_text,
                "prompt_text": prompt_text,
                "tts_model_name": tts_model_name,
                "speech_md5": speech_md5,
                "need_extract_speech": first_text and not have_alloc,
                "stream": True,
                "speech_index": speech_index,
                "semantic_len": semantic_len,
                "bistream": True,
                "append": not first_text,
            }
            if input_data.get("finish", False):
                cur_req_dict["finish"] = True
                cur_req_dict["append"] = True
                await g_objs.httpserver_manager.append_bistream(cur_req_dict, request_id)
                break
            elif first_text:
                generator = g_objs.httpserver_manager.generate(cur_req_dict, request_id, sampling_params)
                process_task = asyncio.create_task(send_wav(websocket, generator))
                first_text = False
            else:
                await g_objs.httpserver_manager.append_bistream(cur_req_dict, request_id)
        await process_task
    except WebSocketDisconnect:
        # å¤„ç†å®¢æˆ·ç«¯æ–­å¼€è¿æ¥
        await g_objs.httpserver_manager.abort(request_id)
    except Exception as e:
        traceback.print_exc()
        await websocket.send_json({"error": f"Server error: {str(e)}"})
    finally:
        await websocket.close()
        if process_task is not None:
            process_task.cancel()


@histogram_timer(request_latency_histogram)
@app.post("/inference_zero_shot")
async def inference_zero_shot(
    request: Request,
    tts_text: str = Form(),
    prompt_text: str = Form(),
    prompt_wav: UploadFile = File(),
    stream: bool = Form(default=False),
    tts_model_name: str = Form(default="default"),
    speed: float = Form(default=1.0),
):
    all_request_counter.inc()
    global lora_styles

    if stream and speed != 1.0:
        return create_error_response(HTTPStatus.BAD_REQUEST, "speed change only support non-stream inference mode")

    if tts_model_name == "default":
        tts_model_name = lora_styles[0]

    # æ£€æŸ¥è¯·æ±‚å‚æ•°
    sample_params_dict = {}
    sampling_params = SamplingParams()
    sampling_params.init(**sample_params_dict)
    sampling_params.verify()
    prompt_text = g_objs.frontend.text_normalize(prompt_text, split=False)
    tts_texts = g_objs.frontend.text_normalize(tts_text, split=True)
    prompt_speech_16k = load_wav(prompt_wav.file, 16000)
    semantic_len = (prompt_speech_16k.shape[1] + 239) // 640 + 10  # + 10 for safe

    prompt_wav.file.seek(0)
    speech_md5 = calculate_md5(prompt_wav.file)

    generate_objs = []
    need_extract_speech = True
    speech_index, have_alloc = g_objs.httpserver_manager.alloc_speech_mem(speech_md5, prompt_speech_16k)

    request_ids = []
    for text in tts_texts:
        cur_req_dict = {
            "text": text,
            "prompt_text": prompt_text,
            "tts_model_name": tts_model_name,
            "speech_md5": speech_md5,
            "need_extract_speech": need_extract_speech and not have_alloc,
            "stream": stream,
            "speech_index": speech_index,
            "semantic_len": semantic_len,
            "speed": speed,
        }
        need_extract_speech = False
        request_id = g_id_gen.generate_id()
        results_generator = g_objs.httpserver_manager.generate(
            cur_req_dict, request_id, sampling_params, request=request
        )
        generate_objs.append(results_generator)
        request_ids.append(request_id)

    print(f"split to request_ids: {request_ids}")
    if stream:
        try:
            return StreamingResponse(generate_data_stream(generate_objs))
        except Exception as e:
            logger.error("An error occurred: %s", str(e), exc_info=True)
            return create_error_response(HTTPStatus.EXPECTATION_FAILED, str(e))
    else:
        try:
            ans_objs = []
            for generator in generate_objs:
                async for result in generator:
                    ans_objs.append(result)
            return StreamingResponse(generate_data(ans_objs))
        except Exception as e:
            logger.error("An error occurred: %s", str(e), exc_info=True)
            return create_error_response(HTTPStatus.EXPECTATION_FAILED, str(e))


@app.post("/query_tts_model")
@app.get("/query_tts_model")
async def show_available_styles(request: Request) -> Response:
    data = {"tts_models": lora_styles}
    json_data = json.dumps(data)
    return Response(content=json_data, media_type="application/json")


@app.get("/metrics")
async def metrics() -> Response:
    metrics_data = generate_latest()
    response = Response(metrics_data)
    response.mimetype = "text/plain"
    return response


@app.on_event("shutdown")
async def shutdown():
    logger.info("Received signal to shutdown. Performing graceful shutdown...")
    await asyncio.sleep(3)

    # æ€æ‰æ‰€æœ‰å­è¿›ç¨‹
    import psutil
    import signal

    parent = psutil.Process(os.getpid())
    children = parent.children(recursive=True)
    for child in children:
        os.kill(child.pid, signal.SIGKILL)
    logger.info("Graceful shutdown completed.")
    return


@app.on_event("startup")
async def startup_event():
    logger.info("=" * 60)
    logger.info("ğŸš€ Application startup: Loading models...")
    loop = asyncio.get_event_loop()
    g_objs.set_args(get_env_start_args())
    loop.create_task(g_objs.httpserver_manager.handle_loop())
    logger.info("âœ… Application ready! Server is now accepting requests")
    logger.info(f"ğŸŒ Listening at: http://{g_objs.args.host}:{g_objs.args.port}")
    logger.info("=" * 60)
    return
